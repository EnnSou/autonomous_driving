\documentclass[10pt,twocolumn,letterpaper]{article}
 \pdfoutput=1
\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bm}
%\usepackage{subcaption}
%\usepackage{slashbox}
\usepackage{color}
\usepackage[english]{babel}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
\usepackage[breaklinks=true,bookmarks=false]{hyperref}


\newcommand{\argmin}{\operatornamewithlimits{argmin}}
\cvprfinalcopy

\def\cvprPaperID{16} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}
\begin{document}
\title{DeepAutoTrack (DAC): Car Trajectory Prediction for Autonomous Driving}

\author{Di Wu, Zhennan Wang, Yi Tang,  Wenbin Zou, Xia Li\\
Shenzhen University\thanks{Shenzhen Key Lab of Advanced Telecommunication and Information Processing, College of Information Engineering, Shenzhen University.}\\
{\tt\small dwu,...@szu.edu.cn}}
\maketitle
%%%%%%%%% ABSTRACT
\begin{abstract}
TODO: abstract
This guide is patterned after my “Doing well in your courses”, a post I wrote a long time ago on some of the tips/tricks I’ve developed during my undergrad. I’ve received nice comments about that guide, so in the same spirit, now that my PhD has come to an end I wanted to compile a similar retrospective document in hopes that it might be helpful to some. Unlike the undergraduate guide, this one was much more difficult to write because there is significantly more variation in how one can traverse the PhD experience. Therefore, many things are likely contentious and a good fraction will be specific to what I’m familiar with (Computer Science / Machine Learning / Computer Vision research). But disclaimers are boring, lets get to it!
\end{abstract}
%%%%%%%%% BODY TEXT
%%% Andrje Karpathy's blog: http://karpathy.github.io/2016/09/07/phd/
%%% Another great resource on this topic is Tips for Writing Technical Papers from Jennifer Widom.
%%% https://cs.stanford.edu/people/widom/paper-writing.html
\section{Introduction}



\textbf{\emph{1. What is the problem?}}

Currently, there are two major paradigms for autonomous driving systems built upon vision-based input~\cite{chen2015deepdriving}: mediated perception approaches that firstly explain the vision input and than parse the scene to make a driving policy (usually by a controller with if-then-else rules), and behaviour reflex approaches that directly map the vision input to a driving policy by a regressor.

In this paper, in the framework of first paradigm, we try to tackle the problem of predicting future trajectories of cars given its past information.

\textbf{\emph{2. Why is it interesting and important?}}

Driver reaction time is a crucial factor for a human driver to react to the traffic participants. Driver reaction time includes recognising the light has changed, deciding to continue or brake, and if stopping engaging the brake (remove foot from accelerator and apply brake). Accident reconstruction specialists commonly use 1.5 seconds~\cite{mcgehee2000driver}.
Therefore, the ability to predict traffic participants' \emph{future} trajectory will greatly benefits the mediated perception approach's driving policy decision making.

\begin{figure}[t]
        \centering
        \includegraphics[width=0.45\textwidth]{figures/pull_figure.pdf}
        \caption{ {\small TODO: Pull figure caption goes here.}}
        \label{fig:response_map}
\end{figure}
\textbf{\emph{3. Why is it hard? (E.g., why do naive approaches fail?)}}

Individually, for computer vision community, it's an unsolved problem. e.g.: object detection, semantic segmentation, instance segmentation, etc.


Currently, most work focus on static images. However, autonomous driving is intrinsically a dynamic problem. Human driver makes decision by sequence of input frames instead of simply one static frame.

\textbf{\emph{4. Why hasn't it been solved before? (Or, what's wrong with previous proposed solutions? How does mine differ?)}}

Scene parsing results contains quite some spurious information.

In traditional vision based tracking problems (\emph{e.g.} ~\cite{wu2013online, wu2015object, mueller2016benchmark}) where target objects go through rather sporadic movements (usually the object trajectory is generated by artificial movement of the target in order to test the robustness of the tracker), the prediction of target's future trajectory is simply a moot problem.
However, traffic participants generally exhibit regular trajectories (e.g., cars driving on the road and pedestrian walking on the sidewalks).

Currently research community lacks proper evaluation metrics for deciding the temporally future prediction result.



\textbf{\emph{5. What are the key components of my approach and results? Also include any specific limitations.}}

In our approach, there are three key components: traffic participants detection, instance tracking and future trajectory prediction.

% Then have a final paragraph or subsection: "Summary of Contributions". It should list the major contributions in bullet form, mentioning in which sections they can be found. This material doubles as an outline of the rest of the paper, saving space and eliminating redundancy.

Our contributions are as follows:
\begin{itemize}

\item We verify the feasibility of predicting the future trajectories of cars for assisting mediated perception framework. The proposed system stems from the issue of human driver reaction time in the autonomous driving context.

\item We present a novel ``tracking-by-detection" framework in the context of autonomous driving to robustly track a car. The proposed framework greatly enhanced the trackers' robustness by updating tracker's target representation, in the meanwhile solving the problem of instance association between frames.

\item We design various temporal model for the problems of predicting future trajectories based on historical data and show that the temporal model generating intermediate output utilising the whole historical data performs better than the frame-to-frame based output temporal model.

\item We demonstrate the effectiveness of using ``privileged information" further boost the prediction accuracy. [NOT DONE].

\item We formalise the car trajectory problem into a 3D occupancy grid problem given depth information.

\item We construct a time-series dataset for car trajectory prediction based on the SYNTHIA dataset~\cite{ros2016synthia}.

\end{itemize}


\section{Related Work}

%%% TY: write related work for object detection, segmentation, instance segmentation
\textbf{\emph{1. Object detection, segmentation, instance segmentation}}
%%% DW: write related work for visual tracking
%%% DW: write related dataset for autonoumous driving

\textbf{\emph{2. Visual trackers}}

%% DW: (1)traditional segmentic segmentation dataset
%% DW: (2) synthetic dataset
%% DW: (3) temporal consistent dataset

\textbf{\emph{3. Dataset for autonoumous driving}}

Camvid dataset~\cite{Camvid} consists of a set of monocular images taken in Cambridge UK. However, only 701 images contain pixel-level annotations over a total of 32 categories (combining objects and architectural scenes).

Similarly, Daimler Urban Segmentation dataset~\cite{scharwachter2013efficient} contains 500 fully labelled monochrome frames for 5 categories.

KITTI benchmark suite~\cite{Geiger2013IJRR} provides a large a mount of images of urban scene from Karlsruhe, Germany, with ground truth data for odometry, object detection, tracking, scene flow and stereo benchmarks. However, a limited 430 labelled images are provided for semantic segmentation.

A common limitation of the aforementioned datasets is the bias introduced by the acquisition of images in a specific city. The LabelMe~\cite{russell2008labelme} project offers the solution by offering around 1,000 fully annotated images of urban environments around the world and more than 3,000 images with partial(noisy) annotations.

More recently, larger project are constructed:
Cityscapes dataset~\cite{Cordts2016Cityscapes} which consists of a collection of images acquired in 50 cities around Germany, Switzerland and France in difference seasons, and having 5,000 images with fine annotations and 20,000 with coarse annotations over a total of 30 classes.

The use of synthetic data has increased considerably in recent years within computer vision community. ~\cite{mueller2016benchmark} provides a synthetic dataset generation and online tracker evaluation. In~\cite{kaneva2011evaluation}, the authors used a photorealistic virtual world to evaluate the performance of image features under scene changes and image transformations. Synthetic data has also been used for skeleton joints estimation~\cite{shotton2013real}, allowing the classifier invariant to pose, body shape, clothing, etc.

A fine annotated images in Cityscape dataset requires on average 1.5 hours which is very labor intensive. Thus, the cost of scaling large project would required a prohibitive economic investment in order to capture images from a larger variety of countries, in different seasons and different traffic conditions. For these reasons, a novel synthetic dataset of urban scene called SYNTHIA~\cite{ros2016synthia} is proposed to use synthetic imagery that simulate real urban scenes in a vast variety of conditions and produce the appropriate annotations.
This dataset is a large collection of images with high variability due to changes in illumination, textures, pose of dynamic objects and camera view-points.

%%% ZN: write related works for Recurrent Networks (used in the context of autonomous driving)
\textbf{\emph{4. Recurrent Networks (used in the context of autonomous driving)}}

Recently, RNN(recurrent network) and its variant LSTM(long short-term memory) have been successfully used in autonomous driving, due to its advantages for modeling sequential data in vision problems. Most of these works exploit LSTM as component of an end-to-end driving model. ~\cite{xu2017end} used the combination of a fully-convolutional network and an LSTM to learn from large-scale vehicle action data, to predict future egomotion including multi-modal discrete and continuous driving behaviors. ~\cite{kim2017interpretable} used a LSTM network to predict the inverse turning radius and generate a heat map of attention at each timestep conditioned on the previous hidden states and a current convolutional feature cube. ~\cite{duself} used 3D convolutional layers to extract visual features, then fed them into LSTM layers to capture the sequential relation. In addition to using deep features, ~\cite{koutnik2013evolving} trained a large recurrent neural network using a reinforcement learning approach to map images directly to steering angles, with the purpose to keep the car on track.


~\cite{xu2017end}: End-to-end learning of driving models from large-scale video dataset.

~\cite{behl2017bounding} Bounding Boxes, Segmentations and Object Coordinates: How Important is Recognition for 3D Scene Flow Estimation in Autonomous Driving Scenarios?

~\cite{chen2015deepdriving} DeepDriving: Learning Affordance for Direct Perception

~\cite{kim2017interpretable} Interpretable Learning for Self-driving cars


\section{Proposed Algorithm}

\subsection{traffic participants detection}


\subsection{instance tracking}

We compare two state-of-the-art trackers (given the time vs. accuracy) of the trackers.

\subsection{trajectory prediction}

We compare two temporal models based on LSTM cells for predicting the car trajectory.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Experiments}

\subsection{Dataset collection}

Currently, most public datasets with semantic labelling are single frame, static images. Continuous video streams are required for the purpose of car trajectory prediction.
We collect car trajectory prediction dataset based on the SYNTHIA~\cite{ros2016synthia} dataset.
SYNTHIA dataset is a large corpus of synthetic images originally collected for the purpose of semantic segmentation of urban scenes generated by rendering a virtual city created with the Unity development platform.
The potential of this virtual world includes extension capabilities: new parts of the cities and road conditions can be easily generated by adding different setups. The major features of collected dataset include: scene diversity (European style town, modern city, highway and green areas), variety of dynamic objects (cars, pedestrians and cyclists), multiple seasons (dedicated themes for winter, fall, spring and summer), lighting conditions and weather (dynamic lights and shadows, several day-time modes, rain mode and night mode).

There are more than 200,000 HD ($760\times1280$) photo-realistic frames from video streams.
Frames are acquired from multiple view-points (up to eight views per location), and each of the frames also contains an associated depth map. In our experiment, we used only left front camera view data.
The focus of this paper is on car trajectory prediction on high ways. We list our logic for collecting continuous car tracking in List~\ref{Dataset_collection}. (TODO: make a list).
Some sample frames with its sementic labels and depth information is shown in Fig. (TODO: TY help to generate the figures).

\begin{algorithm}
\caption{Multimodal Deep Dynamic Networks -- training}\label{Dataset_collection}
\LinesNumbered
\SetAlgoLined
\SetAlgoNoEnd
\DontPrintSemicolon
\SetKwFunction{zeroes}{zeroes}
\KwData{\;
          $ \mathbf{X^1=\{ x^1_i\}_{i \in [1 \ldots t]}}$ - raw input(skeletal) feature sequence.\;
          $ \mathbf{X^2=\{ x^2_i\}_{i \in [1 \ldots t]}}$ - raw input(depth) feature sequence in the form of \;
          \hspace{1cm}$M_1 \times M_2 \times T$, where $M_1, M_2$ are the height and width of the input \;
          \hspace{1cm}  image and $T$ is the number of contiguous frames of the \;
          \hspace{1cm} spatio-temporal cuboid. \;
          \hspace{1cm}  {\scriptsize{}Note that the GPU library \emph{cuda-convnet}~\cite{krizhevsky2012imagenet} used requires square size images and $T$ is a multiple of $4$}.\;
          $ \mathbf{Y=\{ y_i\}_{i \in [1 \ldots t]}}$  - frame based local label (achieved by semi-supervised \; \hspace{1cm} forced-aligment), \;
          \hspace{1cm}  where $ \mathbf{y_i} \in \{ C * S + \textbf{\emph{1}} \} $ with $C$ is the number of class, $S$ is the  \;
          \hspace{1cm}  number of hidden states for each class, $\textbf{\emph{1}}$ as ergodic state.
            }
\For{$m \leftarrow 1$ to $2$}{
    \SetAlgoVlined
    \eIf{$m$ is $1$}{
        Preprocessing the dataCordts2016Cityscapes $ \mathbf{X^1}$ as in Eq.\ref{sk_features}.\;
        Normalizing(zero mean, unit variance per dimension) the above features and feed to to Eq.\ref{GRBMenergy}. \;
        Pre-training the networks using \emph{Contrastive Divergence}. \;
        Supervised fine-tuning the Deep Belief Networks using $ \mathbf{Y}$ by standard mini-batch \emph{SGD} backpropagation.\;
    }{
        Preprocessing the data $ \mathbf{X^2}$ (normalizing, median filtering the depth data) Algo.\ref{normalization_scheme_1} or Algo.\ref{normalization_scheme_2}.\;
        Feeding the above features to Eq.\ref{ReLU}. \;
        Supervised fine-tuning the Deep 3D Convolutional Neural Networks using $ \mathbf{Y}$ by standard mini-batch \emph{SGD} Backpropagation.\;
    }
}
\KwResult{\;
        $\mathbf{GDBN}$ - a gaussian bernoulli visible layer Deep Belief Network to \;
                \hspace{1cm} generate the emission probabilities for hidden markov model.\;
        $\mathbf{3DCNN}$ - a 3D Deep Convolutional Neural Networks to generate the\;
                \hspace{1cm}   emission probabilities for hidden markov model.\;
        $\mathbf{p(H_1)}$ - prior probability for $ \mathbf{Y}$.\;
        $ \mathbf{p(H_t | H_{t-1})}$ - transition probability for $ \mathbf{Y}$,  enforcing the beginning and \;
                    \hspace{1cm} ending of a sequence can only start from the first or the last state.
}
\end{algorithm}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Implementations}

\subsubsection{Traffic participants detection}

We compared two state-of-the-art approaches: SSD~\cite{liu2016ssd} and Faster-RCNN~\cite{ren2015faster} for detecting the traffic participants.
We use the pretrained detection network on the PASCAL VOC detection dataset~\cite{everingham2015pascal} which has 20 classes and fine-tune the network on the SYNTHIA. Curretnly

A comprehensive survey of speed/accuracy trade-offs for modern convolutional object detectors is presented in~\cite{huang2017speed} and we refer the keen readers to


\subsubsection{Instance tracking}

We compare two state-of-the-art trackers (given the time vs. accuracy) of the trackers.

\subsubsection{Trajectory prediction}

We compare two temporal models based on LSTM cells for predicting the car trajectory.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Evaluation}

\begin{table}[t]
\small
   \centering
        \begin{tabular}{|l|| *{2}{c}| *{2}{c} |}\hline
            {\small Configuration} & \multicolumn{2}{|c|}{\small 2D}  & \multicolumn{2}{|c|}{\small 3D}  \\ \hline \hline
                                                    & P20       & AUC       & P20       & AUC  \\ \hline \hline

            {\small MTM }                          &  58.0     &  38.4     & 64.3      & 44.0   \\
            \hline
            {\small \textbf{MTFC} }  &  66.2 ${\color{red}(8.2\uparrow)}$    &  46.1 ${\color{red}(7.7\uparrow)}$    &  71.4 ${\color{red}(7.1\uparrow)}$     &  49.1 ${\color{red}(5.1\uparrow)}$\\
            \hline
            \hline
            {\small MaxRes }                        &  57.5     &  38.8     &  65.8     &  45.9  \\
            \hline
            {\small \textbf{MTM (GT)} }        &  70.6${\color{red}(13.1\uparrow)}$     &  46.7  ${\color{red}(7.9\uparrow)}$   &  77.8 ${\color{red}(12.0\uparrow)}$    &  53.6 ${\color{red}(7.7\uparrow)}$ \\
            \hline
            \hline
            {\small \textbf{MTFC}}           &  74.0${\color{red}(3.4\uparrow)}$      &  51.8${\color{red}(5.1\uparrow)}$      &  79.8${\color{red}(2.0\uparrow)}$      &  57.0${\color{red}(3.4\uparrow)}$    \\
            \hline
            {\small \textbf{DAC}}&  74.2 ${\color{red}(0.2\uparrow)}$    &  52.1${\color{red}(0.3\uparrow)}$     &  79.9${\color{red}(0.1\uparrow)}$     &  57.4${\color{red}(0.4\uparrow)}$\\\hline

        \end{tabular}

    \caption{ {\small
          Baseline comparisons with arrows indicating performance changes comparing with the upper row parallel experiment.
          First two rows investigate the impact of projecting deep features into a kernel space \emph{vs.} using linear deep features.
          Next two rows compare the traditional using max location \emph{vs.} the proposed multi-resolution convnets paradigm for decoding target translational movement.
          Last two rows illustrate the benefit of adaptive learning rate through time (with scale update). }
          } \label{table_baseline}
\end{table}


\begin{table}[t]
\small
   \centering
        \begin{tabular}{|l|| *{2}{c}|}\hline
            {\small Configuration} & \multicolumn{2}{|c|}{\small 2D}    \\ \hline \hline
                                                  & P20       & AUC   \\ \hline \hline

            {\small MTM }                          &  58.0     &  38.4      \\
            \hline
            {\small \textbf{MTFC} }  &  66.2 ${\color{red}(8.2\uparrow)}$    &  46.1 ${\color{red}(7.7\uparrow)}$  \\
            \hline
            \hline
            {\small MaxRes }                        &  57.5     &  38.8   \\
            \hline
            {\small \textbf{MTM (GT)} }        &  70.6${\color{red}(13.1\uparrow)}$     &  46.7  ${\color{red}(7.9\uparrow)}$   \\
            \hline
            \hline
            {\small \textbf{MTFC}}           &  74.0${\color{red}(3.4\uparrow)}$      &  51.8${\color{red}(5.1\uparrow)}$     \\
            \hline
            {\small \textbf{DAC}}&  74.2 ${\color{red}(0.2\uparrow)}$    &  52.1${\color{red}(0.3\uparrow)}$    \\\hline

        \end{tabular}

    \caption{ {\small
          Comparison with the most recent methods }
          } \label{table_baseline2}
\end{table}
\section{Conclusions}

Future works include conducting experiment on the real-world images.

\section*{Details of the Code}


\section*{Acknowledgment}

{\small
\bibliographystyle{ieee}
\bibliography{egbib}
}
\end{document}
